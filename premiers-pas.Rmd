---
title: "Premiers pas"
author: "Timothée Flutre"
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
---

<!--
Ce morceau de code R est utilisé pour vérifier que tout ce dont on a besoin est disponible.
-->
```{r setup, include=FALSE}
R.v.maj <- as.numeric(R.version$major)
R.v.min.1 <- as.numeric(strsplit(R.version$minor, "\\.")[[1]][1])
if(R.v.maj < 2 || (R.v.maj == 2 && R.v.min.1 < 15))
  stop("requires R >= 2.15", call.=FALSE)

suppressPackageStartupMessages(library(knitr))
opts_chunk$set(echo=TRUE, warning=TRUE, message=TRUE, cache=FALSE)

suppressPackageStartupMessages(library(MASS))

options(digits=3)
```


# Préambule

Ce document a été généré à partir d'un fichier texte au format Rmd utilisé avec le logiciel libre [R](http://www.r-project.org/).
Pour exporter un tel fichier vers les formats HTML et PDF, installez le paquet [rmarkdown](http://cran.r-project.org/web/packages/rmarkdown/index.html) disponible sur CRAN (il va vraisemblablement vous être demandé d'installer d'autres paquets), puis ouvrez R et entrez:
```{r ex_rmd, eval=FALSE}
library(rmarkdown)
render("myanalysis.Rmd", "all")
```

Il est généralement plus simple d'utiliser le logiciel libre [RStudio](http://www.rstudio.com/), mais ce n'est pas obligatoire.
Pour plus de détails, lisez [cette page](http://rmarkdown.rstudio.com/).
Pour écrire des équations avec LaTeX, reportez-vous au [livre en ligne](https://fr.wikibooks.org/wiki/LaTeX).


# Contexte

Ce document fait partie de l'atelier "Prédiction Génomique" organisé et animé par Jacques David et Timothée Flutre en 2015 à [Montpellier SupAgro](http://www.supagro.fr) dans le cadre de l'option [APIMET](http://www.agro-montpellier.fr/web/pages/?idl=19&page=216&id_page=630) (Amélioration des Plantes et Ingénierie végétale Méditerranéennes et Tropicales) couplée à la spécialité SEPMET (Semences Et Plants Méditerranéens Et Tropicaux) du [Master 3A](http://www.supagro.fr/web/pages/?idl=19&page=1689) (Agronomie et Agroalimentaire).

Ce document a pour but d'introduire concrètement les étudiants à l'un des aspects de la modélisation statistique, la simulation.
Il prend comme exemple la régression linéaire simple, historiquement développée par [Galton (1886)](http://dx.doi.org/10.2307/2841583) pour étudier l'hérédité de la taille dans l'espèce humaine.


# Introduction

## A propos de la modélisation

"Essentially, all models are wrong, but some are useful." (Box, 1987).
Cette célèbre citation illustre parfaitement le fait qu'un modèle est une simplification du phénomène étudié, mais qu'après tout, si cette simplification nous apporte des enseignements et nous permet de prendre de bonnes décisions, cela importe tout autant.

Il est donc important de rappeler que la première question à se poser, en tant que modélisateur, concerne la validité du modèle.
Bien que cela paraisse évident, ceci consiste avant tout à vérifier que les données à analyser correspondent bien à la question à laquelle on veut répondre ([Gelman & Hill, 2006](http://www.worldcat.org/isbn/0521867061)).

Il est également utile, pour mieux comprendre le processus de modélisation statistique, de distinguer le "monde réel", dans lequel vivent les données, du "monde théorique", dans lequel vivent les modèles: "When we use a statistical model to make a statistical inference, we implicitly assert that the variation exhibited by data is captured reasonably well by the statistical model, so that the theoretical world corresponds reasonably well to the real world." ([Kass, 2011](http://dx.doi.org/10.1214/10-sts337)).

En particulier, il ne faut pas confondre les données avec des variables aléatoires, même si on fait souvent le raccourci: "In both approaches [frequentist and Bayesian], a statistical model is introduced and we may say that the inference is based on what *would* happen if the data *were* to be random variables distributed according to the statistical model. This modeling assumption would be reasonable if the model *were* to describe accurately the variation in the data." ([Kass, 2011](http://dx.doi.org/10.1214/10-sts337)).


## Notations et vocabulaire

L'inférence avec un modèle statistique consiste généralement à *estimer* les paramètres, puis à s'en servir pour *prédire* de nouvelles données.

Lorsque l'on propose un modèle pour répondre à une question donnée, on commence donc par expliquer les notations.
Suivant les conventions, nous utilisons des lettres grecques pour dénoter les paramètres (non-observés), par exemple $\theta$, des lettres romaines pour les données observées, par exemple $y$, et des lettres romaines surmontées d'un tilde pour les données prédites, $\tilde{y}$.
L'ensemble des paramètres est généralement noté en majuscule, par exemple $\Theta = \{ \theta_1, \theta_2 \}$.
De plus, s'il y a plusieurs paramètres ou données, ils se retrouvent mathématiquement dans des vecteurs, en gras, ce qui donne $\boldsymbol{\theta}$ et $\boldsymbol{y}$.

Une fois les notations établies, on écrit la *vraisemblance* (likelihood en anglais), souvent présentée comme étant la "probabilité des données sachant les paramètres".
En fait, si les données sont des variables continues, c'est la densité de probabilité des données sachant les paramètres, notée $p(y | \theta)$, et si les données sont des variables discrètes, c'est la fonction de masse, notée $P(y | \theta)$.
Mais le plus important est de réaliser que, dans la vraisemblance, ce ne sont pas les données qui varient, mais les paramètres: la vraisemblance est une fonction des paramètres, d'où le fait qu'on la note $\mathcal{L}(\theta)$ ou $\mathcal{L}(\theta | y)$.

Tout naturellement, la méthode du *maximum de vraisemblance* cherche à identifier la valeur du paramètre, notée $\hat{\theta}$ par convention, qui maximise la vraisemblance:

\begin{align}
\hat{\theta} = \text{argmax}_\theta \, \mathcal{L} \; \; \Leftrightarrow \; \; \frac{\partial \mathcal{L}}{\partial \theta}(\hat{\theta}) = 0
\end{align}

Mais ceci n'est que le tout début de la démarche!
En effet, certains décrivent les statistiques comme étant la *science de l'incertitude* ([Lindley, 2000](http://dx.doi.org/10.1111/1467-9884.00238)).
Or pour l'instant, nous n'avons parlé que de la façon d'obtenir *une* valeur par paramètre, celle qui maximise la vraisemblance.
Il est donc primordial ensuite de *quantifier l'incertitude* que nous avons quant à cette valeur.
C'est sur ce point que différentes approches sont possibles (*fréquentiste*, *bayésienne*).
Mais quelle que soit l'approche, on peut généralement s'attendre au fait que, plus le nombre d'échantillons augmente (s'ils sont "représentatifs), plus l'incertitude sur la valeur du paramètre diminue.


## Comprendre par l'exemple

Le vocabulaire esquissé au paragraphe précédent n'est pas forcément très intuitif.
Par exemple, supposons que l'on étudie une quantité physique dont la valeur résulte de la somme d'une très grande quantité de facteurs indépendants, chacun ayant un faible impact sur la valeur finale.
Prenons trois mesures de cette quantité d'intérêt.
Comme il y a de la variation, on choisit d'introduire une *variable aléatoire* $Y$ correspondant à la quantité d'intérêt, et dénotons par $y_1$, $y_2$ et $y_3$ les trois observations, vues comme des réalisations de cette variable aléatoire.

```{r echo=FALSE}
set.seed(1)
y <- rnorm(n=3, mean=5, sd=1)
```
Par exemple, supposons les valeurs suivantes:

- $y_1$=`r y[1]`

- $y_2$=`r y[2]`

- $y_3$=`r y[3]`

Etant donné les caractéristiques du phénomène ("somme d'une très grande quantité de facteurs indépendants, chacun ayant un faible impact"), il est raisonnable de supposer que la variable $Y$ suit une *loi Normale* (c.f. le théorème central limite).
Cette distribution de probabilité est caractérisée par deux paramètres, sa *moyenne* que l'on note généralement $\mu$, et sa *variance* que l'on note généralement $\sigma^2$ ($\sigma$ étant l'écart-type).
En terme de notation, on écrit $Y \sim \mathcal{N}(\mu, \sigma^2)$, et la densité de probabilité de la réalisation $y$ de $Y$ s'écrit:
\begin{align}
Y \sim \mathcal{N}(\mu, \sigma^2) \; \; \Leftrightarrow \; \; p(Y=y | \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y - \mu)^2}{2 \sigma^2} \right)
\end{align}

L'intérêt de ce modèle paramétrique est de pouvoir *résumer* les données, par exemple un million de mesures, par seulement deux valeurs, les paramètres.
Mais bien entendu, nous ne connaissons pas les valeurs de paramètres!
La moyenne $\mu$ peut prendre toutes les valeurs entre $-\infty$ et $+\infty$, et la variance $\sigma^2$ n'a pour seule restriction que d'être positive.
Or comme le montrent les graphiques ci-dessous, la loi Normale peut être assez différente selon les valeurs de ces paramètres:

```{r effect_mu, echo=FALSE}
x <- seq(from=-3, to=10, length.out=500)
plot(x=x, y=dnorm(x=x, mean=2, sd=1), type="l", col="blue",
     main="Comparaison de deux lois Normale (sigma=1)",
     xlab="quantile", ylab="densité de probabilité")
points(x=x, y=dnorm(x=x, mean=5, sd=1), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c("mu=2", "mu=5"), col=c("blue", "red"), lty=1, bty="n")
```

```{r effect_sigma, echo=FALSE}
x <- seq(from=-7, to=13, length.out=500)
plot(x=x, y=dnorm(x=x, mean=3, sd=1), type="l", col="blue",
     main="Comparaison de deux lois Normale (mu=3)",
     xlab="quantiles", ylab="densité de probabilité")
points(x=x, y=dnorm(x=x, mean=3, sd=3), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c("sigma=1", "sigma=3"), col=c("blue", "red"), lty=1, bty="n")
```

Revenons à nos trois mesures: `r y`.
Parmi toutes les valeurs possibles des paramètres, le défi consiste donc à trouver celles pour lesquelles la loi Normale est une bonne description du mécanisme qui a généré ces données.
On parle donc de trouver les valeurs des paramètres de telle sorte que la *vraisemblance* d'obtenir ces données soit la plus élevée possible pour ces valeurs des paramètres.

Pour rendre les choses plus facile, supposons que l'on connaisse déjà la variance: $\sigma^2 = 1$.
Il ne nous reste plus qu'à trouver la moyenne, $\mu$.
Regardons cela de plus près pour la première observation, $y_1$=`r y[1]`:

```{r ex_lik_y1, echo=FALSE}
x <- seq(from=-3, to=10, length.out=500)
plot(x=x, y=dnorm(x=x, mean=2, sd=1), type="l", col="blue",
     main="",
     xlab="quantiles", ylab="densité de probabilité")
points(x=x, y=dnorm(x=x, mean=5, sd=1), type="l", col="red")
abline(v=0, lty=2)
legend("topright", legend=c("mu=2", "mu=5"), col=c("blue", "red"), lty=1, bty="n")
segments(x0=y[1], y0=0, x1=y[1], y1=dnorm(x=y[1], mean=2, sd=1))
segments(x0=y[1], y0=0, x1=y[1], y1=dnorm(x=y[1], mean=5, sd=1))
segments(x0=y[1], y0=dnorm(x=y[1], mean=2, sd=1),
         x1=-5, y1=dnorm(x=y[1], mean=2, sd=1), col="blue")
segments(x0=y[1], y0=dnorm(x=y[1], mean=5, sd=1),
         x1=-5, y1=dnorm(x=y[1], mean=5, sd=1), col="red")
```

D'après le graphique, la densité de probabilité $p(y_1 | \mu=5, \sigma=1)$ est strictement supérieure à $p(y_1 | \mu=2, \sigma=1)$.
Cela se vérifie si l'on fait le calcul avec la formule ci-dessus: $p(y_1 | \mu=5, \sigma=1)$=`r dnorm(x=y[1], mean=5, sd=1)` et $p(y_1 | \mu=2, \sigma=1)$=`r dnorm(x=y[1], mean=2, sd=1)`.
Comme les deux densités ont la même valeur pour $\sigma$, la différence vient bien du terme $(y - \mu)^2$ dans l'exponentielle, terme qui représente l'écart à la moyenne.
Au final, nous pouvons donc dire qu'en ce qui concerne la première observation, la vraisemblance $\mathcal{L}(\mu=5,\sigma=1)$ est plus grande que $\mathcal{L}(\mu=2,\sigma=1)$.

Comme l'on dispose de plusieurs observations et que l'on suppose qu'elles sont toutes des réalisations de la même variable aléatoire, il est pertinent de calculer la vraisemblance de toutes ces observations conjointement plutôt que séparément:
\[
\mathcal{L}(\mu, \sigma) = p(y_1,y_2,y_3 | \mu, \sigma)
\]

Si l'on fait aussi l'hypothèse que ces observations sont indépendantes, cela se simplifie en:
\begin{align*}
\mathcal{L}(\mu, \sigma) &= p(y_1 | \mu, \sigma) \times p(y_2 | \mu, \sigma) \times p(y_3 | \mu, \sigma) \\
&= \prod_{i=1}^3 p(y_i | \mu, \sigma)
\end{align*}

Il n'est pas très pratique de maximiser la vraisemblance directement, on préfère donc passer au log (qui est monotone, donc le maximum de l'un est le maximum de l'autre):
\begin{align*}
l(\mu,\sigma) &= \log \mathcal{L}(\mu, \sigma) \\
&= \sum_{i=1}^3 \log p(y_i | \mu, \sigma) \\
&= \sum_{i=1}^3 \log \left[ \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y_i - \mu)^2}{2 \sigma^2} \right) \right] \\
&= - 3 \log \sigma \; - \frac{3}{2} \log (2\pi) \; - \frac{1}{2 \sigma^2} \sum_{i=1}^3 (y_i - \mu)^2
\end{align*}

Lorsque le nombre d'observations augmente, un simple examen graphique n'est pas très pratique ni suffisant.
D'où le fait, qu'en pratique, (i) l'on écrive une fonction qui calcule la log-vraisemblance, et (ii) l'on cherche le maximum de cette fonction:

```{r compute_lik}
compute.log.likelihood <- function(parameters, data){
  mu <- parameters[1]
  sigma <- parameters[2]
  y <- data
  n <- length(y)
  log.lik <- - n * log(sigma) - (n/2) * log(2 * pi) - sum(((y - mu)^2) / (2 * sigma^2))
  return(log.lik)
}

compute.log.likelihood(c(5,1), y)
compute.log.likelihood(c(2,1), y)
```

Dans le cas de la loi Normale, il existe déjà dans R des fonctions implémentant la densité de probabilité, ce qui nous permet de vérifier que nous n'avons pas fait d'erreur:
```{r check_compute_lik}
sum(dnorm(x=y, mean=5, sd=1, log=TRUE))
sum(dnorm(x=y, mean=2, sd=1, log=TRUE))
```

Cette section devrait vous avoir donné les bases du raisonnement ainsi que des techniques que nous allons utiliser dans la suite de l'atelier, commençant dès maintenant avec la régression linéaire simple.


# Ecrire le modèle


## Notations

* $n$: nombre d'individus (diploïdes, supposés non-apparentés)

* $i$: indice indiquant le $i$-ème individu, donc $i \in \{1,\ldots,n\}$

* $y_i$: phénotype de l'individu $i$ pour la caractère d'intérêt

* $\mu$: moyenne globale du phénotype des $n$ individus

* $f$: fréquence de l'allèle minoritaire au marqueur SNP d'intérêt (situé sur un autosome)

* $x_i$: génotype de l'individu $i$ à ce SNP, codé comme le nombre de copie(s) de l'allèle minoritaire à ce SNP chez cet individu ($\forall i \; x_i \in \{0,1,2\}$)

* $\beta$: effet additif de chaque copie de l'allèle minoritaire en unité du phénotype

* $\epsilon_i$: erreur pour l'individu $i$

* $\sigma^2$: variance des erreurs


## Vraisemblance

Dans notre cas, nous supposons que le génotype au SNP d'intérêt a un effet additif sur la moyenne du phénotype, ce qui s'écrit généralement:
\[
\forall i \; \; y_i = \mu + \beta x_i + \epsilon_i \text{ avec } \epsilon_i \overset{\text{i.i.d}}{\sim} \mathcal{N}(0, \sigma^2)
\]

Une autre façon, mais équivalente, de l'écrire est:
\[
\forall i \; \; y_i | x_i, \mu, \beta, \sigma \; \overset{\text{i.i.d}}{\sim} \mathcal{N}(\mu + \beta x_i, \sigma^2)
\]

Cette deuxième formulation montre bien que, de manière générale, si l'on connaît les valeurs des variables explicatives ainsi que celles des paramètres, alors on est capable de *simuler* des valeurs de réponse.

De plus, on peut écrire la vraisemblance sous forme plus explicite comme une fonction des paramètres $\Theta = \{ \mu, \beta, \sigma \}$:

\begin{align*}
\mathcal{L}(\Theta) &= p(\boldsymbol{y} | \boldsymbol{x}, \Theta) = p(y_1,\ldots,y_n | x_1,\ldots,x_n, \mu, \beta, \sigma) \\
&= \prod_{i=1}^n p(y_i | x_i, \mu, \beta, \sigma) \\
&= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y_i - \mu - \beta x_i)^2}{2 \sigma^2} \right)
\end{align*}


# Simuler des données

Afin de simuler des données, nous allons utiliser un générateur de nombres pseudo-aléatoires.
Le mot "pseudo" est là pour rappeler que les générateurs informatiques sont déterministes et peuvent donc être initialisés avec une graine (*seed*), très utile pour la reproducibilité des analyses:

```{r set_seed}
seed <- 1859
set.seed(seed)
```

Commençons par fixer le nombre d'individus:
```{r set_sample_size}
n <- 500
```

Puis la moyenne globale (de manière arbitraire, ce n'est pas très important car on peut toujours centrer les phénotypes en début d'analyse):
```{r set_global_mean}
mu <- 50
```

Pour simuler les génotypes, nous allons supposer que la population est à l'équilibre d'Hardy-Weinberg:
```{r simul_geno}
##' Calculate the genotype frequencies (AA, Aa, aa) at Hardy-Weinberg equilibrium.
##'
##' https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle
##' @param maf frequency of the minor allele, a
##' @return vector of genotype frequencies
##' @author Timothée Flutre
calc.geno.freq <- function(maf){
  geno.freq <- c((1 - maf)^2,
                2 * (1 - maf) * maf,
                maf^2)
  names(geno.freq) <- c("AA", "Aa", "aa")
  return(geno.freq)
}

f <- 0.3
genotypes <- sample(x=c(0,1,2), size=n, replace=TRUE, prob=calc.geno.freq(f))
```

Le morceau de code ci-dessus vous montre aussi les bonnes pratiques de programmation:

* choisir des noms de fonctions et variables clairs et explicites;

* documenter le code;

* citer des référence si nécessaire.

Regardons à quoi ressemblent les génotypes que nous venons de simuler:
```{r look_geno}
table(genotypes)
sum(genotypes) / (2 * n) # estimate of the MAF
var(genotypes) # important for the estimate of beta
```

Tirons une valeur pour l'effet du génotype sur le phénotype:
```{r simul_geno_effect}
(beta <- rnorm(n=1, mean=1, sd=1))
```

Simulons des erreurs (par simplicité, fixons $\sigma$ à 1):
```{r simul_errors}
sigma <- 1
errors <- rnorm(n=n, mean=0, sd=sigma)
```

Nous avons maintenant tout ce qu'il faut pour simuler les phénotypes:
```{r simul_pheno}
phenotypes <- mu + beta * genotypes + errors
```

Regardons à quoi ils ressemblent:
```{r look_pheno}
hist(phenotypes, breaks="FD", las=1, main="",
     xlab="Phenotypes", ylab="Number of individuals")
```

Comme ce qui nous intéresse ici, ce ne sont pas uniquement les phénotypes, mais bien la relation entre les génotypes et les phénotypes, un autre type de graphique semble plus approprié:
```{r look_geno_pheno}
boxplot(phenotypes ~ genotypes, xlab="Genotypes", ylab="Phenotypes", las=1)
```

Remarquez l'importance de faire des graphiques les plus clairs et intelligibles possible.
Ce ne sont souvent que les graphiques que l'on montre à la place des tableaus de résultats, ceux-ci étant plus difficiles à lire ([Gelman *et al.*, 2002](http://dx.doi.org/10.1198/000313002317572790)).

Pour la suite, il est habituel dans R d'organiser les données dans un tableau:
```{r org_data}
dat <- data.frame(x=genotypes, y=phenotypes)
summary(dat)
```


# Réaliser l'inférence

## Dérivation mathématique

Pour trouver les valeurs des paramètres qui maximisent la vraisemblance, on travaille généralement avec la log-vraisemblance, $l(\Theta) = \log \mathcal{L}(\Theta)$, puis on utilise les [règles d'analyse](https://en.wikipedia.org/wiki/Differentiation_rules):

* $\frac{\partial l}{\partial \beta}(\hat{\beta}) = 0 \; \Leftrightarrow \; \hat{\beta} = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2}$

* $\frac{\partial l}{\partial \mu}(\hat{\mu}) = 0 \; \Leftrightarrow \; \hat{\mu} = \bar{y} - \hat{\beta} \bar{x}$

* $\frac{\partial l}{\partial \sigma}(\hat{\sigma}) = 0 \; \Leftrightarrow \; \hat{\sigma} = \frac{1}{n} \sum_i (y_i - (\hat{\mu} + \hat{\beta} x_i))^2$

où $\bar{x} = \frac{1}{n} \sum_i x_i$ et $\bar{y} = \frac{1}{n} \sum_i y_i$.

En plus de ces estimations, il est fréquent d'évaluer la qualité du modèle en calculant la proportion de la variance des $\{y_i\}$ expliquée par les $\{x_i\}$:

$R^2 = \frac{\sum_i ((\hat{\mu} + \hat{\beta} x_i) - \bar{y})^2}{\sum_i (y_i - \bar{y})^2}$



## Implémentation (R stats)

Bien entendu, R a nativement une fonction implémentant l'estimation par maximum de vraisemblance d'un modèle linéaire:
```{r fit}
fit <- lm(y ~ x, data=dat)
```

Avant toute autre chose, il nous faut vérifier que les hypothèses du modèles sont vérifiées (homoscédasticité, Normalité, indépendence):
```{r diagnostics}
par(mfrow=c(2,2))
plot(fit)
```

Ceci semble être bien le cas (évidemment puisque nous avons simulé nous-même les données!).

Nous pouvons donc extraire de l'objet renvoyé par la fonction lm() les quantités qui nous intéressent et les comparer aux vraies valeurs utilisées pour simuler les données:
```{r get_results}
summary(fit)
c(coefficients(fit)[1], coefficients(fit)[2], summary(fit)$sigma)
c(mu, beta, sigma)
summary(fit)$r.squared
```

On peut aussi ajouter la droite de régression au graphique des données:
```{r look_geno_pheno_reg}
plot(phenotypes ~ jitter(genotypes, factor=0.5), las=1,
     xlab="Genotypes", ylab="Phenotypes")
abline(a=coefficients(fit)[1], b=coefficients(fit)[2], col="red")
legend("bottomright", legend="MLE line", col="red", lty=1, bty="n")
```


## Implémentation (R base)

Il peut être intéressant, surtout dans ce cas simple, d'implémenter cette méthode par soi-même pour mieux la comprendre.
Pour cela, il faut d'abord écrire une fonction à qui on fournit les données et les paramètres et qui renvoie la log-vraisemblance.
Puis on peut utiliser la fonction [mle](http://stat.ethz.ch/R-manual/R-devel/library/stats4/html/mle.html) du paquet stats4.

TODO


# Explorer les simulations possibles

La simulation est un outil particulièrement utile pour explorer comment un modèle répond à des changements dans les données et les paramètres.

Maintenant, c'est à vous: que voudriez-vous explorer en premier?


# Perspectives

## Améliorer le modèle

Naturellement, l'activité de modélisation statistique ne se limite pas à simuler des données sur ordinateur.
Bien au contraire, elle est au coeur de l'activité de recherche en ce qu'elle vise à identifier les caractéristiques saillantes d'un phénomène naturel afin d'en réaliser l'inférence.

Concernant le thème de l'atelier, la prédiction génomique, quelles sont les limites du modèle exploré ci-dessus?
Que proposez-vous pour y remédier?

## Ré-écriture du modèle

Dans une section précédente, nous avons vu que la vraisemblance pouvait s'écrire de la manière suivante:

\[
\forall i \; \; y_i | x_i, \mu, \beta, \sigma \; \overset{\text{i.i.d}}{\sim} \mathcal{N}(\mu + \beta x_i, \sigma^2)
\]

La loi Normale ci-dessus est utilisée sous sa forme *univariée*, c'est-à-dire qu'on s'en sert pour tirer aléatoirement un seul nombre correspondant à une réalisation de la variable aléatoire d'intérêt.

Or la même loi existe aussi sous forme *multivariée*, ce qui permet, à chaque fois qu'on "tire" dedans, d'obtenir *plusieurs* nombres, ceux-ci étant arrangés dans un vecteur.
Sous cette forme, la vraisemblance s'écrit maintenant:

\[
\boldsymbol{y} | \boldsymbol{x}, \mu, \beta, \sigma \; \sim \mathcal{N}_n(\mu \boldsymbol{1} + \beta \boldsymbol{x}, \sigma^2 I_n)
\]

où $\boldsymbol{y}$ est le vecteur de dimension $n$ contenant les phénotypes, $\boldsymbol{x}$ est le vecteur de dimension $n$ contenant les génotypes, $\boldsymbol{1}$ est le vecteur de dimension $n$ ne contenant que des $1$, et $I_n$ est la matrice identité également de dimension $n$.

D'ailleurs, on écrit souvent ce type de vraisemblance de manière encore plus concise:

\[
\boldsymbol{y} | X, \boldsymbol{\beta}, \sigma \; \sim \mathcal{N}_n(X \boldsymbol{\beta}, \Sigma)
\]

où $X$ est une matrice à deux colonnes ($\boldsymbol{1}$ et $\boldsymbol{x}$), $\boldsymbol{\beta}$ est un vecteur à deux élements ($\mu$ et $\beta$) et $\Sigma$ est une matrice $n \times n$ égale à $\sigma^2 I_n$.

Sous cette forme, ne devient-il pas "facile" d'inclure dans le modèle certains élements proposés en perspective?


# Annexe

```{r info}
print(sessionInfo(), locale=FALSE)
```
