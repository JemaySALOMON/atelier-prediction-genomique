---
title: "Prédiction génomique"
author: "Timothée Flutre"
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
---

<!--
Ce morceau de code R est utilisé pour vérifier que tout ce dont on a besoin est disponible.
-->
```{r setup, include=FALSE}
R.v.maj <- as.numeric(R.version$major)
R.v.min.1 <- as.numeric(strsplit(R.version$minor, "\\.")[[1]][1])
if(R.v.maj < 2 || (R.v.maj == 2 && R.v.min.1 < 15))
  stop("requires R >= 2.15", call.=FALSE)

suppressPackageStartupMessages(library(knitr))
opts_chunk$set(echo=TRUE, warning=TRUE, message=TRUE, cache=FALSE)

suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(rrBLUP))
suppressPackageStartupMessages(library(BGLR))
suppressPackageStartupMessages(library(data.table))

options(digits=3)
```


# Contexte

Ce document fait partie de l'atelier "Prédiction Génomique" organisé et animé par Jacques David et Timothée Flutre en 2015 à [Montpellier SupAgro](http://www.supagro.fr) dans le cadre de l'option [APIMET](http://www.agro-montpellier.fr/web/pages/?idl=19&page=216&id_page=630) (Amélioration des Plantes et Ingénierie végétale Méditerranéennes et Tropicales) couplée à la spécialité SEPMET (Semences Et Plants Méditerranéens Et Tropicaux) du [Master 3A](http://www.supagro.fr/web/pages/?idl=19&page=1689) (Agronomie et Agroalimentaire).

Ce document a pour but d'explorer par simulation l'intérêt de la prédiction génomique en sélection artificielle pour la création variétale.
Il est recommandé d'avoir déjà lu et suivi le document "Premiers pas" de l'atelier.


# Introduction

Modèle standard de la génétique quantitative (voir les références en fin de document):
\begin{align}
y_i = g_i + \epsilon_i
\end{align}

* $i$: indice du $i$ème individu parmi les $N$ qui composent l'échantillon ($i \in \{1,\ldots,N\}$)

* $y_i$: valeur phénotypique de l'individu $i$, considérée comme continue

* $g_i$: valeur génotypique de l'individu $i$ (peut être interprétée comme la valeur phénotypique moyenne de l'individu s'il est cloné dans tous les environnements possibles)

* $\epsilon_i$: composante non-génétique pour l'individu $i$ ("déviation environnementale")

Quel que soit l'individu, si l'on suppose que ses valeur génotypique et composante non-génétique ne sont pas corrélées, alors sa variance phénotypique est égale à $\sigma_p^2 = \sigma_g^2 + \sigma_\epsilon^2$.
A ce stade, l'héritabilité au sens large est définit comme étant $H^2 = \frac{\sigma_g^2}{\sigma_g^2 + \sigma_\epsilon^2}$.

La valeur génotypique peut également se décomposer en composantes additive, de dominance et d'épistasie: $g_i = a_i + d_i + \zeta_i$.
On suppose généralement aussi que ces composantes ne sont pas corrélées, et donc $\sigma_g^2 = \sigma_a^2 + \sigma_d^2 + \sigma_\zeta^2$.
Ceci amène à définir l'héritabilité au sens strict par $h^2 = \frac{\sigma_a^2}{\sigma_g^2 + \sigma_\epsilon^2}$.

Le même modèle, mais en notation matricielle:
\begin{align}
\boldsymbol{y} = \boldsymbol{g} + \boldsymbol{\epsilon}
\end{align}

* $G$: matrice de variance-covariance $N \times N$ des valeurs génotypiques;

* $R$: matrice de variance-covariance $N \times N$ des composantes non-génétiques.

La matrice $G$, dite "d'apparentement", se décompose aussi en relations additives, de dominance et d'épistasie, même si les premières sont généralement les seules utilisées en pratique.
Dans ce cas, $G = \sigma_a^2 A$ où $\sigma_a^2$ est estimé et $A$, la matrice des relations génétiques additives, est calculée à partir de l'arbre généalogique (pédigrée) des individus.
De plus, la matrice $R$ est généralement diagonale, telle que $R = \sigma_\epsilon^2 I$ où $\sigma_\epsilon^2$ est estimé simultanément à $\sigma_a^2$, et $I$ est la matrice identité.

Si l'on suppose que $\boldsymbol{g} \sim \mathcal{N}_N(\boldsymbol{0}, G)$ et $\boldsymbol{\epsilon} \sim \mathcal{N}_N(\boldsymbol{0}, R)$, alors $\hat{\boldsymbol{g}} = E[\boldsymbol{g} | \boldsymbol{y}] = G (G + R)^{-1} \boldsymbol{y}$ où $\hat{\boldsymbol{g}}$ est le meilleur prédicteur linéaire sans biais de $\boldsymbol{g}$ (Best Linear Unbiased Predictor, BLUP) et $H = G (G + R)^{-1}$ est une généralisation matricielle de l'héritabilité.

Or il faut bien remarquer que la généalogie ne permet que de calculer la matrice d'apparentement *attendue*, celle-ci pouvant donc différer de la matrice d'apparentement *réalisée*.
En effet, bien qu'en moyenne le coefficient d'apparentement (identité par descendance) entre un allèle d'un parent et un allèle de son enfant soit de 1/4, cette proportion varie le long du génome, à cause, entre autres, de l'échantillonage mendélien des chromosomes et de la variation du taux de recombinaison le long des chromosomes.
De plus, la généalogie seule ne permet pas d'identifier quelles régions du génomes ont une variation génotypique plus ou moins associée à la variation phénotypique, les fameux locus influençant les traits quantitatifs (Quantitative Trait Locus, QTL).

Si maintenant l'on dispose des génotypes $\{\boldsymbol{x}_i\}$ à un ensemble de $P$ marqueurs génétiques, le modèle devient:
\begin{align}
y_i = g(\boldsymbol{x}_i, \boldsymbol{\beta}) + \epsilon_i
\end{align}
On peut n'utiliser les marqueurs que pour estimer $G$ ($A$) plus précisément, mais on peut aussi inclure directement les marqueurs comme variables explicatives dans le modèle.
Les marqueurs sont souvent des SNP dont le génotype est codé en terme de dosage allélique ($x_{ij} \in \{0,1,2\}$).
Comme précédemment, seuls les effets additifs sont généralement pris en compte et la valeur génotypique s'écrit alors $g(\boldsymbol{x}_i, \boldsymbol{\beta}) = \sum_{j=1}^P x_{ij} \beta_j$.

Avec le toujours plus grand débit des technologies des séquençage, il est très fréquent qu'il y ait beaucoup plus de marqueurs que d'individus: $P >> N$.
Dans ce tels cas, la méthode traditionnelle du maximum de vraisemblance présentée dans le document "Premiers pas" ne donne plus de bonnes estimations des effets des marqueurs, les $\{\beta_j\}$, et doit être "pénalisée" (on dit aussi "régularisée").
On parle encore de "rétrécir" les estimations des effets (shrinkage en anglais).
Dans l'approche bayésienne, que $P >> N$ ou non, c'est simplement le prior sur les $\{\beta_j\}$ qui change.

Explicitement incorporer les effets de dominance et d'épistatie entre marqueurs semble infaisable étant donné l'explosion combinatoire qui en résulte.
Certains auteurs ont donc proposé des modèles semi-paramétriques (espace de Hilbert à noyau reproduisant, RKHS en anglais; réseaux neuronaux).

Quoi qu'il en soit, une abondance d'articles existe sur ces sujets (voir les revues listées en fin de document) et, pour se familiariser avec ces questions à moindre coût, rien de mieux que de faire des simulations!


# Simuler des génotypes et des phénotypes

## Modèle

L'un des modèles les plus utilisés est la régression d'arête (ridge regression en anglais):
\[
\boldsymbol{y} = \boldsymbol{1} \mu + X \boldsymbol{\beta} + \boldsymbol{\epsilon} \; \; \text{ avec } \boldsymbol{\beta}, \boldsymbol{\epsilon} | \sigma_\beta^2, \sigma_\epsilon^2 \; \sim \; \mathcal{N}_N(\boldsymbol{0}, \sigma_\beta^2 I) \mathcal{N}_N(\boldsymbol{0}, \sigma_\epsilon^2 I)
\]
où $Var(\boldsymbol{y}) = \sigma_\beta^2 X X' + \sigma_\epsilon^2 I$.

Comme vous pouvez le voir, ce modèle présuppose que le caractère d'intérêt a une architecture génétique où tous les marqueurs ont un effet, que donc, vraisemblablement, ces effets sont faibles, et qu'ils s'additionnent les uns aux autres.
On parle de "modèle additif infinitésimal".

Il est d'ailleurs possible de montrer qu'il est équivalent au modèle dit "animal" de l'ère pré-marqueurs:
\[
\boldsymbol{y} = \boldsymbol{1} \mu' + Z \boldsymbol{u} + \boldsymbol{\epsilon} \; \; \text{ avec } \boldsymbol{u}, \boldsymbol{\epsilon} | \sigma_a^2, \sigma_\epsilon^2 \; \sim \; \mathcal{N}_N(\boldsymbol{0}, \sigma_a^2 A) \mathcal{N}_N(\boldsymbol{0}, \sigma_\epsilon^2 I)
\]
où $Z$ n'est qu'une matrice d'incidence (ici l'identité), le vecteur $\boldsymbol{u}$ contient les valeurs génotypiques additives, aussi appelées valeurs en croisement (breeding values en anglais), et $Var(\boldsymbol{y}) = \sigma_a^2 A + \sigma_\epsilon^2 I$.
Avec quelques hypothèses et en supposant un nombre infini de marqueurs, la matrice $A$ est égale à $X X' / [2 \sum_{j=1}^P p_j (1 - p_j)]$ où $p_j$ est la fréquence de l'allèle minoritaire au SNP $j$.

Dans cette section, choisissons donc le modèle de régression d'arête avec les notations suivantes:

* $N$: nombre d'individus ($i \in \{1,\ldots,N\}$);

* $\boldsymbol{y}$: vecteur de longueur $N$ des valeurs phénotypiques;

* $\mu$: valeur phénotypique moyenne;

* $W$: matrice de dimension $N \times Q$ d'incidence de covariables;

* $\boldsymbol{\alpha}$: vecteur de longueur $Q$ des effets de ces covariables (modélisés comme "fixes" dans la terminologie traditionnelle);

* $X$: matrice de dimension $N \times P$ de codage des génotypes aux marqueurs;

* $\boldsymbol{\beta}$: vecteur de longueur $P$ des effets de ces génotypes (modélisés comme "aléatoires" dans la terminologie traditionnelle);

* $\boldsymbol{\epsilon}$: vecteur de longueur $N$ des erreurs;

* $\sigma_\beta^2$: composant de la variance contribué par $\boldsymbol{\beta}$;

* $\sigma_\epsilon^2$: composant de la variance contribué par $\boldsymbol{\epsilon}$.

La vraisemblance s'écrit généralement comme suit:
\[
\boldsymbol{y} = \boldsymbol{1} \mu + W \boldsymbol{\alpha} + X \boldsymbol{\beta} + \boldsymbol{\epsilon}
\]
avec, dans l'approche fréquentiste, les distributions suivantes pour les effets aléatoires:

* $\boldsymbol{\beta} \sim \mathcal{N}_N(\boldsymbol{0}, \sigma_\beta^2 I)$;

* $\boldsymbol{\epsilon} \sim \mathcal{N}_N(\boldsymbol{0}, \sigma_\epsilon^2 I)$;

* $Cov(\boldsymbol{\beta}, \boldsymbol{\epsilon}) = 0$.

Dans l'approche bayésienne, on l'écrira généralement de la façon suivante:
\[
\boldsymbol{y} | \mu, \boldsymbol{\alpha}, \boldsymbol{\beta}, \sigma_\beta^2, \sigma_\epsilon^2 \; \sim \mathcal{N}_N(\boldsymbol{1} \mu + W \boldsymbol{\alpha} + X \boldsymbol{\beta}, \sigma_\epsilon^2 I)
\]

avec comme paramètres $\Theta = \{ \mu, \boldsymbol{\alpha}, \boldsymbol{\beta}, \sigma_\beta^2, \sigma_\epsilon^2 \}$.
Plusieurs priors sont possibles, mais choisissons la simplicité avec $p(\Theta) = p(\mu) p(\boldsymbol{\alpha}) p(\boldsymbol{\beta} | \sigma_\beta^2) p(\boldsymbol{\epsilon} | \sigma_\epsilon^2) p(\sigma_\beta^2) p(\sigma_\epsilon^2)$ où:

* $\mu \sim \mathcal{N}_N(\boldsymbol{0}, 10^{10} I)$, un prior "plat "(flat en anglais);

* $\boldsymbol{\alpha} \sim \mathcal{N}_N(\boldsymbol{0}, 10^{10} I)$, idem;

* $\boldsymbol{\beta} | \sigma_\beta^2 \; \sim \mathcal{N}_N(\boldsymbol{0}, \sigma_\beta^2 I)$;

* $\boldsymbol{\epsilon} | \sigma_\epsilon^2 \; \sim \mathcal{N}_N(\boldsymbol{0}, \sigma_\epsilon^2 I)$;

* $\sigma_\beta^2 \sim \chi^{-2}(\nu=5, \tau^2=1)$;

* $\sigma_\epsilon^2 \sim \chi^{-2}(\nu=5, \tau^2=1)$.

TODO: faire des graphiques explorant les distributions *a priori*


## Simulation

```{r simul_ridge-reg}
seed <- 1859; set.seed(seed)
N <- 1000
inds.id <- sprintf(fmt=paste0("ind%0", floor(log10(N))+1, "i"), 1:N)
mu <- 54
Q <- 1
W <- matrix(data=rnorm(N*Q, mean=12, sd=2), nrow=N, ncol=Q,
            dimnames=list(inds.id, paste0("fix", 1:Q)))
(alpha <- rnorm(n=Q, mean=5, sd=1))
P <- 5000
snps.id <- sprintf(fmt=paste0("snp%0", floor(log10(P))+1, "i"), 1:P)

##' Calculate the genotype frequencies (AA, Aa, aa) at Hardy-Weinberg equilibrium.
##'
##' https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle
##' @param maf frequency of the minor allele, a
##' @return vector of genotype frequencies
calc.geno.freq <- function(maf=0.3){
  geno.freq <- c((1 - maf)^2,
                2 * (1 - maf) * maf,
                maf^2)
  names(geno.freq) <- c("AA", "Aa", "aa")
  return(geno.freq)
}

X <- matrix(sample(x=c(0,1,2), size=N*P, replace=TRUE, prob=calc.geno.freq()),
            nrow=N, ncol=P,
            dimnames=list(inds.id, snps.id))
table(X)

A <- A.mat(X-1)
A[1:3,1:3]

##' Plot a matrix as an image in the right orientation.
##'
##' @param M matrix to plot
##' @return none
image.mat <- function(M, ...){
  image(t(M)[,nrow(M):1], ...)
}

image.mat(A)

beta <- rep(0, P)
nb.qtls <- P # all markers have an effect
sigma.beta2 <- 1
beta[sample.int(P, nb.qtls)] <- rnorm(n=nb.qtls, mean=0, sd=sigma.beta2)

##' Estimate the minor allele frequency of a SNP whose genotypes are coded in allele dose.
##'
##' @param x vector of {0,1,2}
##' @return numeric
estim.maf.1SNP <- function(x){
  stopifnot(is.vector(x))
  N <- length(x)
  f.hat <- sum(x) / (2 * N)
  if(f.hat > 0.5)
    f.hat <- 1 - f.hat
  return(f.hat)
}

##' Estimate the minor allele frequencies of many SNPs whose genotyped are coded in allele dose.
##'
##' @param X matrix with individuals in rows and SNPs in columns
##' @return vector of numeric
estim.maf.allSNPs <- function(X){
  stopifnot(is.matrix(X))
  P <- ncol(X)
  out <- rep(NA, P)
  names(out) <- colnames(X)
  for(j in 1:P)
    out[j] <- estim.maf.1SNP(X[,j])
  return(out)
}

mafs <- estim.maf.allSNPs(X)
(sigma.a2 <- var(X %*% beta)[1,1]) # var of additive relationships
(sigma.beta2 * 2 * sum(mafs * (1 - mafs)))
h2 <- 0.60
(sigma.epsilon2 <- ((1 - h2) / h2) * sigma.a2)
epsilon <- matrix(mvrnorm(n=1, mu=rep(0, N), Sigma=sigma.epsilon2 * diag(N)))
y <- matrix(1, nrow=N) * mu + W %*% alpha + X %*% beta + epsilon
summary(y)
hist(y, breaks="FD", main="", las=1,
     xlab="Phenotypes", ylab="Number of individuals")
```


## Inférence

### En fréquentiste

Paquet [rrBLUP](http://cran.r-project.org/web/packages/rrBLUP/index.html):

```{r fit_ridge-reg_rrBLUP}
system.time(
    fit.rrBLUP <- mixed.solve(y=y, Z=X, X=cbind(rep(1,N), W),
                              method="REML", SE=TRUE, return.Hinv=TRUE)
)
cbind(c(mu, alpha), fit.rrBLUP$beta)
fit.rrBLUP$beta.SE
c(sigma.epsilon2, fit.rrBLUP$Ve)
c(sigma.beta2, fit.rrBLUP$Vu)
cor(beta, fit.rrBLUP$u)
```

### En bayésien

En bref, il suffit d'appliquer la formule de Bayes:
\begin{align}
p(\Theta | \boldsymbol{y}, W, X) &= \frac{p(\Theta, \boldsymbol{y} | W, X)}{p(\boldsymbol{y} | W, X)} \\
&= \frac{p(\Theta) p(\boldsymbol{y} | W, X, \Theta)}{\int_\Theta p(\Theta) p(\boldsymbol{y} | W, X, \Theta) \text{d}\Theta} \\
&\propto p(\Theta) p(\boldsymbol{y} | W, X, \Theta)
\end{align}

Cette formule est la simple application de la définition de la [probabilité conditionnelle](https://fr.wikipedia.org/wiki/Probabilit%C3%A9_conditionnelle), ainsi que de la formule des [probabilités totales](https://fr.wikipedia.org/wiki/Formule_des_probabilit%C3%A9s_totales).
Pour mieux retenir la formule de Bayes, remarquez que le posterior est proportionnel au prior multiplié par la vraisemblance.

Paquet [BGLR](http://cran.r-project.org/web/packages/BGLR/index.html):

```{r fit_ridge-reg_BGLR}
system.time(
    fit.BGLR <- BGLR(y=y, ETA=list(list(X=W, model="FIXED"),
                              list(X=X, model="BRR")),
                     verbose=FALSE, nIter=7000, burnIn=1000, thin=5)
)
c(mu, fit.BGLR$mu)
fit.BGLR$SD.mu
c(alpha, fit.BGLR$ETA[[1]]$b)
fit.BGLR$ETA[[1]]$SD.b
c(sigma.epsilon2, fit.BGLR$varE)
fit.BGLR$SD.varE
c(sigma.beta2, fit.BGLR$ETA[[2]]$varB)
cor(beta, fit.BGLR$ETA[[2]]$b)
```

Lorsque l'on utilise un algorithme MCMC, il est toujours important de vérifier qu'il a convergé, au moins visuellement:
```{r fit_simple_BGLR_short_plots}
plot(scan("varE.dat"), las=1,
     xlab="Iterations (after burn-in and thinning)",
     ylab=expression(paste("Samples of ", sigma[epsilon]^2)))
abline(h=fit.BGLR$varE, col="red")
hist(scan("varE.dat"), breaks="FD", main="", las=1,
     xlab=expression(sigma[epsilon]^2), ylab="Number of samples")
abline(v=fit.BGLR$varE, col="red")
abline(v=sigma.epsilon2, col="red", lty=2)
legend("topright", legend=c("posterior mean","true value"),
       col="red", lty=c(1,2), bty="n")

plot(scan("ETA_2_varB.dat"), las=1,
     xlab="Iterations (after burn-in and thinning)",
     ylab=expression(paste("Samples of ", sigma[beta]^2)))
abline(h=fit.BGLR$ETA[[2]]$varB, col="red")
hist(scan("ETA_2_varB.dat"), breaks="FD", main="", las=1,
     xlab=expression(sigma[beta]^2), ylab="Number of samples")
abline(v=fit.BGLR$ETA[[2]]$varB, col="red")
abline(v=sigma.beta2, col="red", lty=2)
legend("topleft", legend=c("posterior mean","true value"),
       col="red", lty=c(1,2), bty="n")

plot(beta, fit.BGLR$ETA[[2]]$b, asp=1, las=1,
     xlab=expression(paste("True values of ", beta[j])),
     ylab=expression(paste("Posterior means of ", beta[j])))
abline(lm(fit.BGLR$ETA[[2]]$b ~ beta), col="red")
```


### Comparaison

Les deux approches donnent des résultats très similaires concernant les effets des marqueurs:
```{r comp_ridge-reg_rrBLUP_BGLR}
plot(fit.rrBLUP$u, fit.BGLR$ETA[[2]]$b, asp=1, las=1,
     xlab=expression(paste("Estimated values of ", beta[j], " by rrBLUP")),
     ylab=expression(paste("Estimated values of ", beta[j], " by BGLR")))
abline(lm(fit.BGLR$ETA[[2]]$b ~ fit.rrBLUP$u), col="red")
```


## Critiques

Il est important de réaliser à quel point la simulation ci-dessus est simpliste.
Tout d'abord, les génotypes ont été simulés indépendamment les uns des autres, sans tenir compte de l'apparentement entre individus ni du déséquilibre de liaison entre marqueurs adjacents le long du génome.
Pour remédier à cela, le plus simple serait de simuler des génotypes via le "coalescent avec recombinaison" (voir paquet [scrm](http://cran.r-project.org/web/packages/scrm/index.html)).

De plus, tous les caractères n'ont pas une architecture génétique additive et infinitésimale.
Dans certains cas (la majorité?), seul un sous-ensemble des marqueurs a un effet.
D'autres priors doivent alors être utilisés, qui visent à sélectionner les marqueurs à effet non-nul, par exemple "BayesB" dans la fonction BGLR().

Par ailleurs, certains caractères ne sont pas toujours bien capturés par la loi Normale, comme les caractères discrets, qu'ils soient binaires (résistant versus sensible) ou non (nombre de grains par épi).

Quoi qu'il en soit, afin de se rapprocher de cas réels, commençons par utiliser de vrais génotypes.


# Simuler des phénotypes à partir de vrais génotypes

Commençons par charger le fichier au format CSV avec les noms des SNP en première ligne, les noms des individus en première colonne, et le dosage alléliques dans le reste:

```{r load_true_genos, eval=TRUE}
## system.time(X <- read.csv("true_genotypes.csv", header=TRUE, nrows=381))
## system.time(
##     X <- read.csv(unz("true_genotypes.csv.zip", "true_genotypes.csv"),
##                   header=TRUE, row.names=1, nrows=381,
##                   colClasses=c("character", rep("integer", 62220)))
## )
system.time(tmp <- as.data.frame(fread("true_genotypes.csv")))
X <- as.matrix(tmp[,-1])
rownames(X) <- tmp[,1]
dim(X)
X[1:3,1:5]
str(X[1:3,1:5])
```

TODO: tracer la distribution des fréquences alléliques

Calculer les relations génétiques additives et les visualiser:
```{r plot_add-rel, eval=TRUE}
A <- A.mat(X - 1)
A[1:3,1:3]
image.mat(A, xlab="Individuals", ylab="Individuals")
```

TODO: définir différents sous-ensembles de marqueurs


# Prédire grâce au génome

Inspirez-vous du [tutoriel](http://pbgworks.org/node/1440) d'Amy Jacobson (Univ. Minnesota); le pdf est [ici](http://pbgworks.org/sites/pbgworks.org/files/Introduction%20to%20Genomic%20Selection%20in%20R.pdf).


# Explorer des jeux de données réels librement disponibles

Comme l'a fait justement remarquer Zamir (PLoS Biology 2013, Science 2014), il est très difficile de trouver des jeux de données avec phénotypes en libre accès.
Cependant, en voici quelques uns:

* [Crossa *et al* (Genetics, 2010)](http://dx.doi.org/10.1534/genetics.110.118521): blé (599 lignées, 4 conditions, rendement en grains, pédigrée, 1279 marqueurs DArT) et maïs (300 lignées, 1148 marqueurs SNP, 3 caractères, deux conditions)

* [Resende *et al* (Genetics, 2012)](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3316659/): pin (951 individus de 61 familles, pédigrée, 4853 marqueurs SNP, phénotypes dérégréssés)

* [Cleveland *et al* (G3, 2012)](http://dx.doi.org/10.1534/g3.111.001453): porc (3534 animaux, pédigrée, 5 caractères, 53000 marqueurs SNP)


# Perspectives

Au-delà des critiques faites dans la section 3, les deux plus grandes simplifications de ce travail ont été de ne se concentrer que sur un seul caractère, et de complètement ignorer l'environnement.

Or c'est bien là le défi des sélectionneurs, qu'ils soient dans des entreprises semencières ou dans des collectifs de paysans: créer de nouvelles variétés combinant plusieurs caractères d'intérêt et adaptées à l'itinéraire technique, à la filière économique, à l'agriculteur et au consommateur, etc.

Mais ce sera pour un autre cours!


# Références

* Barton, N. H. and P. D. Keightley (2002, January). Understanding quantitative genetic variation. Nature Reviews Genetics 3 (1), 11-21. [DOI](http://dx.doi.org/10.1038/nrg700)

* Weir, B. S., A. D. Anderson, and A. B. Hepler (2006, October). Genetic relatedness analysis: modern data and new challenges. Nature Reviews Genetics 7 (10), 771-780. [DOI](http://dx.doi.org/10.1038/nrg1960)

* Visscher, P. M., W. G. Hill, and N. R. Wray (2008, March). Heritability in the genomics era — concepts and misconceptions. Nature Reviews Genetics 9 (4), 255-266. [DOI](http://dx.doi.org/10.1038/nrg2322)

* Slatkin, M. (2008, June). Linkage disequilibrium — understanding the evolutionary past and mapping the medical future. Nature Reviews Genetics 9 (6), 477-485. [DOI](http://dx.doi.org/10.1038/nrg2361)

* Stephens, M. and D. J. Balding (2009, October). Bayesian statistical methods for genetic association studies. Nature Reviews Genetics 10 (10), 681-690. [DOI](http://dx.doi.org/10.1038/nrg2615)

* de los Campos, G., D. Gianola, and D. B. Allison (2010, December). Predicting genetic predisposition in humans: the promise of whole-genome markers. Nature Reviews Genetics 11 (12), 880-886. [DOI](http://dx.doi.org/10.1038/nrg2898)

* Morrell, P. L., E. S. Buckler, and J. Ross-Ibarra (2012, February). Crop genomics: advances and applications. Nature Reviews Genetics 13 (2), 85-96. [DOI](http://dx.doi.org/10.1038/nrg3097)


# Annexe

```{r info}
print(sessionInfo(), locale=FALSE)
```
