---
title: "Régression linéaire simple"
author: "Timothée Flutre"
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
---

<!--
Ce morceau de code R est utilisé pour vérifier que tout ce dont on a besoin est disponible.
-->
```{r setup, include=FALSE}
R.v.maj <- as.numeric(R.version$major)
R.v.min.1 <- as.numeric(strsplit(R.version$minor, "\\.")[[1]][1])
if(R.v.maj < 2 || (R.v.maj == 2 && R.v.min.1 < 15))
  stop("requires R >= 2.15", call.=FALSE)

suppressPackageStartupMessages(library(knitr))
opts_chunk$set(echo=TRUE, warning=TRUE, message=TRUE, cache=FALSE)

suppressPackageStartupMessages(library(MASS))

options(digits=3)
```


# Préambule

Ce document a été généré à partir d'un fichier texte au format Rmd utilisé avec le logiciel libre [R](http://www.r-project.org/).
Pour exporter un tel fichier vers les formats HTML et PDF, installez le paquet [rmarkdown](http://cran.r-project.org/web/packages/rmarkdown/index.html) disponible sur CRAN (il va vraisemblablement vous être demandé d'installer d'autres paquets), puis ouvrez R et entrez:
```{r ex_rmd, eval=FALSE}
library(rmarkdown)
render("myanalysis.Rmd", "all")
```

Il est généralement plus simple d'utiliser le logiciel libre [RStudio](http://www.rstudio.com/), mais ce n'est pas obligatoire.
Pour plus de détails, lisez [cette page](http://rmarkdown.rstudio.com/).
Pour écrire des équations avec LaTeX, reportez-vous au [livre en ligne](https://fr.wikibooks.org/wiki/LaTeX).


# Contexte

Ce document fait partie de l'atelier "Prédiction Génomique" organisé et animé par Jacques David et Timothée Flutre en 2015 à [Montpellier SupAgro](http://www.supagro.fr) dans le cadre de l'option [APIMET](http://www.agro-montpellier.fr/web/pages/?idl=19&page=216&id_page=630) (Amélioration des Plantes et Ingénierie végétale Méditerranéennes et Tropicales) couplée à la spécialité SEPMET (Semences Et Plants Méditerranéens Et Tropicaux) du [Master 3A](http://www.supagro.fr/web/pages/?idl=19&page=1689) (Agronomie et Agroalimentaire).

Ce document a pour but d'introduire concrètement les étudiants à l'un des aspects de la modélisation statistique, la simulation.
Il prend comme exemple la régression linéaire simple, historiquement développée par [Galton (1886)](http://dx.doi.org/10.2307/2841583) pour étudier l'hérédité de la taille dans l'espèce humaine.


# Introduction

"Essentially, all models are wrong, but some are useful." (Box, 1987).
Cette célèbre citation illustre parfaitement le fait qu'un modèle est une simplification du phénomène étudié, mais qu'après tout, si cette simplification nous apporte des enseignements et nous permet de prendre de bonnes décisions, cela importe tout autant.

Il est donc important de rappeler que la première question à se poser, en tant que modélisateur, concerne la validité du modèle.
Bien que cela paraisse évident, ceci consiste avant tout à vérifier que les données à analyser correspondent bien à la question à laquelle on veut répondre ([Gelman & Hill, 2006](http://www.worldcat.org/isbn/0521867061)).

Il est également utile, pour mieux comprendre le processus de modélisation statistique, de distinguer le "monde réel", dans lequel vivent les données, du "monde théorique", dans lequel vivent les modèles: "When we use a statistical model to make a statistical inference, we implicitly assert that the variation exhibited by data is captured reasonably well by the statistical model, so that the theoretical world corresponds reasonably well to the real world." ([Kass, 2011](http://dx.doi.org/10.1214/10-sts337)).

En particulier, il ne faut pas confondre les données avec des variables aléatoires, même si on fait souvent le raccourci: "In both approaches [frequentist and Bayesian], a statistical model is introduced and we may say that the inference is based on what *would* happen if the data *were* to be random variables distributed according to the statistical model. This modeling assumption would be reasonable if the model *were* to describe accurately the variation in the data." ([Kass, 2011](http://dx.doi.org/10.1214/10-sts337)).


# Ecrire le modèle

## Rappel

L'inférence avec un modèle statistique consiste généralement à estimer les paramètres, puis à s'en servir pour prédire de nouvelles données.

Lorsque l'on propose un modèle pour répondre à une question donnée, on commence donc par expliquer les notations.
Suivant les conventions, nous utilisons des lettres grecques pour dénoter les paramètres (non-observés), par exemple $\theta$, des lettres romaines pour dénoter les données observées, $y$, et un tilde pour les données prédites, $\tilde{y}$.
L'ensemble des paramètres est généralement noté en majuscule, $\Theta$.
De plus, s'il y a plusieurs paramètres ou données, on utilise des vecteurs notés en gras, ce qui donne $\boldsymbol{\theta}$ et $\boldsymbol{y}$.

Une fois les notations établies, on écrit la vraisemblance (*likelihood*), souvent présentée comme étant la "probabilité des données sachant les paramètres".
En fait, si les données sont des variables continues, c'est la densité de probabilité des données sachant les paramètres, notée $p(y | \theta)$, et si les données sont des variables discrètes, c'est la fonction de masse, notée $P(y | \theta)$.
Mais le plus important est de réaliser que, dans la vraisemblance, ce ne sont pas les données qui varient, mais les paramètres: la vraisemblance est une fonction des paramètres, d'où le fait qu'on la note $\mathcal{L}(\theta)$.

Tout naturellement, la méthode du maximum de vraisemblance cherche donc à identifier la valeur du paramètre, notée $\hat{\theta}$, qui maximise la vraisemblance.

\[
\hat{\theta} = \text{argmax}_\theta \, \mathcal{L} \; \; \Leftrightarrow \; \; \frac{\partial \mathcal{L}}{\partial \theta}(\hat{\theta}) = 0
\]


## Notations

* $n$: nombre d'individus (diploïdes, supposés non-apparentés)

* $i$: indice indiquant le $i$-ème individu, donc $i \in \{1,\ldots,n\}$

* $y_i$: phénotype de l'individu $i$ pour la caractère d'intérêt

* $\mu$: moyenne globale du phénotype des $n$ individus

* $f$: fréquence de l'allèle minoritaire au marqueur SNP d'intérêt (situé sur un autosome)

* $x_i$: génotype de l'individu $i$ à ce SNP, codé comme le nombre de copie(s) de l'allèle minoritaire à ce SNP chez cet individu ($\forall i \; x_i \in \{0,1,2\}$)

* $\beta$: effet additif de chaque copie de l'allèle minoritaire en unité du phénotype

* $\epsilon_i$: erreur pour l'individu $i$

* $\sigma^2$: variance des erreurs

* $\mathcal{N}(\mu,\sigma^2)$: distribution Normale univariée de moyenne $\mu$ et variance $\sigma^2$

## Vraisemblance

Dans notre cas, nous supposons que le génotype au SNP d'intérêt a un effet additif sur la moyenne du phénotype, ce qui s'écrit généralement:
\[
\forall i \; \; y_i = \mu + \beta x_i + \epsilon_i \text{ avec } \epsilon_i \overset{\text{i.i.d}}{\sim} \mathcal{N}(0, \sigma^2)
\]

Une autre façon, mais équivalente, de l'écrire est:
\[
\forall i \; \; y_i | x_i, \mu, \beta, \sigma \; \overset{\text{i.i.d}}{\sim} \mathcal{N}(\mu + \beta x_i, \sigma^2)
\]

A partir de cela, il devient maintenant facile de *simuler des données*: si vous choisissez des valeurs pour les $x_i$, ainsi que pour les paramètres, vous pouvez facilement générer des valeurs pour les $y_i$.

De plus, on peut écrire la vraisemblance sous forme plus explicite de fonction des paramètres $\Theta = \{ \mu, \beta, \sigma\}$:

\begin{align*}
\mathcal{L}(\Theta) &= p(\boldsymbol{y} | \boldsymbol{x}, \Theta) = p(y_1,\ldots,y_n | x_1,\ldots,x_n, \mu, \beta, \sigma) \\
&= \prod_{i=1}^n p(y_i | x_i, \mu, \beta, \sigma) \\
&= \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(y_i - \mu - \beta x_i)^2}{2 \sigma^2} \right)
\end{align*}

Pour trouver les valeurs des paramètres qui maximisent la vraisemblance, on travaille généralement avec la log-vraisemblance, $l(\Theta) = \log \mathcal{L}(\Theta)$, puis on utilise les [règles d'analyse](https://en.wikipedia.org/wiki/Differentiation_rules) pour calculer $\frac{\partial l}{\partial \beta}$, etc.


# Simuler des données

Afin de simuler des données, nous allons utiliser un générateur de nombres pseudo-aléatoires.
Le mot "pseudo" est là pour rappeler que les générateurs informatiques sont déterministes et peuvent donc être initialisés avec une graine (*seed*), très utile pour la reproducibilité des analyses:

```{r set_seed}
seed <- 1859
set.seed(seed)
```

Commençons par fixer le nombre d'individus:
```{r set_sample_size}
n <- 500
```

Puis la moyenne globale (de manière arbitraire, ce n'est pas très important car on peut toujours centrer les phénotypes en début d'analyse):
```{r set_global_mean}
mu <- 50
```

Pour simuler les génotypes, nous allons supposer que la population est à l'équilibre d'Hardy-Weinberg:
```{r simul_geno}
##' Calculate the genotype frequencies (AA, Aa, aa) at Hardy-Weinberg equilibrium.
##'
##' https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle
##' @param maf frequency of the minor allele, a
##' @return vector of genotype frequencies
##' @author Timothée Flutre
calc.geno.freq <- function(maf){
  geno.freq <- c((1 - maf)^2,
                2 * (1 - maf) * maf,
                maf^2)
  names(geno.freq) <- c("AA", "Aa", "aa")
  return(geno.freq)
}

f <- 0.3
genotypes <- sample(x=c(0,1,2), size=n, replace=TRUE, prob=calc.geno.freq(f))
```

Le morceau de code ci-dessus vous montre aussi les bonnes pratiques de programmation:

* choisir des noms de fonctions et variables clairs et explicites;

* documenter le code;

* citer des référence si nécessaire.

Regardons à quoi ressemblent les génotypes que nous venons de simuler:
```{r look_geno}
table(genotypes)
sum(genotypes) / (2 * n)
var(genotypes)
```

Tirons une valeur pour l'effet du génotype sur le phénotype:
```{r simul_geno_effect}
(beta <- rnorm(n=1, mean=1, sd=1))
```

Simulons des erreurs:
```{r simul_errors}
sigma <- 1
errors <- rnorm(n=n, mean=0, sd=sigma)
```

Nous avons maintenant tout ce qu'il faut pour simuler les phénotypes:
```{r simul_pheno}
phenotypes <- mu + beta * genotypes + errors
```

Regardons à quoi ils ressemblent:
```{r look_pheno}
hist(phenotypes, breaks="FD")
```

Comme ce qui nous intéresse ici, ce ne sont pas uniquement les phénotypes, mais bien la relation entre les génotypes et les phénotypes, un autre type de graphique semble plus approprié:
```{r look_geno_pheno}
boxplot(phenotypes ~ genotypes, xlab="genotypes", ylab="phenotypes")
```

Pour la suite, il est habituel dans R d'organiser les données dans un tableau:
```{r org_data}
dat <- data.frame(x=genotypes, y=phenotypes)
summary(dat)
```


# Réaliser l'inférence

## Dérivation mathématique

TODO

## Implémentation (R stats)

Bien entendu, R a nativement une fonction implémentant l'estimation par maximum de vraisemblance d'un modèle linéaire:
```{r fit}
fit <- lm(y ~ x, data=dat)
```

Avant toute autre chose, il nous faut vérifier que les hypothèses du modèles sont vérifiées (homoscédasticité, Normalité, indépendence):
```{r diagnostics}
par(mfrow=c(2,2))
plot(fit)
```

Ceci semble être bien le cas (évidemment puisque nous avons simulé nous-même les données...).

Nous pouvons donc extraire de l'objet les quantités qui nous intéresse et les comparer aux vraies valeurs utilisées pour simuler les données:
```{r get_results}
summary(fit)
c(mu, beta, sigma)
(mu.hat <- coefficients(fit)[1])
(beta.hat <- coefficients(fit)[2])
(sigma.hat <- sqrt((1/(n-2) * sum(fit$residuals^2))))
```


## Implémentation (R base)

Il peut être intéressant, surtout dans ce cas simple, d'implémenter cette méthode par soi-même, histoire de mieux la comprendre.
Pour cela, il faut d'abord écrire une fonction R à qui on donne les données et les valeurs de paramètres et qui renvoie la valeur de la log-vraisemblance.
Puis on peut utiliser la fonction [mle](http://stat.ethz.ch/R-manual/R-devel/library/stats4/html/mle.html) du paquet stats4.

TODO


# Explorer les simulations possibles

La simulation est un outil particulièrement utile pour explorer comment un modèle répond à des changements dans les données.

Maintenant, c'est à vous: que voudriez-vous explorer en premier?


# Perspectives

Naturellement, l'activité de modélisation statistique ne se limite pas à simuler des données sur ordinateur.
Bien au contraire, elle est au coeur de l'activité de recherche en ce qu'elle vise à identifier les caractéristiques saillantes d'un phénomène naturel afin d'en réaliser l'inférence.

Concernant le thème de l'atelier, la prédiction génomique, quelles sont les limites du modèle exploré ci-dessus?
Que proposez-vous pour y remédier?




# Annexe

```{r info}
print(sessionInfo(), locale=FALSE)
```
